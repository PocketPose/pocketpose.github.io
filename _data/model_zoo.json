[
    {
        "name": "Lightweight Pose Network",
        "description": "Recent research on human pose estimation has achieved significant improvement. However, most existing methods tend to pursue higher scores using complex architecture or computationally expensive models on benchmark datasets, ignoring the deployment costs in practice. In this paper, we investigate the problem of simple and lightweight human pose estimation. We first redesign a lightweight bottleneck block with two non-novel concepts: depthwise convolution and attention mechanism. And then, based on the lightweight block, we present a Lightweight Pose Network (LPN) following the architecture design principles of SimpleBaseline. The model size (#Params) of our small network LPN-50 is only 9% of SimpleBaseline(ResNet50), and the computational complexity (FLOPs) is only 11%. To give full play to the potential of our LPN and get more accurate predicted results, we also propose an iterative training strategy and a model-agnostic post-processing function Beta-Soft-Argmax. We empirically demonstrate the effectiveness and efficiency of our methods on the benchmark dataset: the COCO keypoint detection dataset. Besides, we show the speed superiority of our lightweight network at inference time on a non-GPU platform. Specifically, our LPN-50 can achieve 68.7 in AP score on the COCO test-dev set, with only 2.7M parameters and 1.0 GFLOPs, while the inference speed is 17 FPS on an Intel i7-8700K CPU machine.",
        "pipeline": "Top-down",
        "dimension": "2d",
        "backbone": "ResNet + Lightweight Bottleneck Blocks",
        "head": "Heatmap Regression",
        "paper": "https://arxiv.org/abs/1911.10346",
        "authors": "Zhe Zhang, Jie Tang, Gangshan Wu",
        "year": "2019",
        "type": "body2d",
        "variants": {
            "50": {
                "name": "LPN-50 (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=13pzu7J6FJnroqG_lHibQyV5sA6aHuOwD",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1dldLwjOacXV_uGkbxfEIPPJEK_2A-Snp/view?usp=sharing",
                "code": "https://github.com/zhang943/lpn-pytorch/blob/master/experiments/coco/lpn/lpn50_256x192_gd256x2_gc.yaml",
                "license": "MIT"
            },
            "101": {
                "name": "LPN-101 (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1ZX7DJKfimTt6p8GaH_0JHIH1h5sXh_aK",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1sMMgZreTwVsNSwgJDgkJ06wMIdpC5JjJ/view?usp=sharing",
                "code": "https://github.com/zhang943/lpn-pytorch/blob/master/experiments/coco/lpn/lpn101_256x192_gd256x2_gc.yaml",
                "license": "MIT"
            },
            "152": {
                "name": "LPN-152 (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1xgZFPGg1kh0w2cgGwF7hbDiwBpltGoO2",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1yDSs58ZjG_M5JWonKQkVaFBsTqoUC-o5/view?usp=sharing",
                "code": "https://github.com/zhang943/lpn-pytorch/blob/master/experiments/coco/lpn/lpn152_256x192_gd256x2_gc.yaml",
                "license": "MIT"
            },
            "50m": {
                "name": "LPN-50 (MPII)",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "mpii",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1AwvdMTTEm5quwltcCofvfQ1EBDEp_C0Q",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1lQmpmbHyW-_tXIFpY3yraaQC83mF-sLf/view?usp=sharing",
                "code": "https://github.com/zhang943/lpn-pytorch/blob/master/experiments/mpii/lpn/lpn50_256x256_gd256x2_gc.yaml",
                "license": "MIT"
            }
        }
    },
    {
        "name": "BlazePose",
        "description": "We present BlazePose, a lightweight convolutional neural network architecture for human pose estimation that is tailored for real-time inference on mobile devices. During inference, the network produces 33 body keypoints for a single person and runs at over 30 frames per second on a Pixel 2 phone. This makes it particularly suited to real-time use cases like fitness tracking and sign language recognition. Our main contributions include a novel body pose tracking solution and a lightweight body pose estimation neural network that uses both heatmaps and regression to keypoint coordinates.",
        "pipeline": "Top-down",
        "dimension": "3d",
        "backbone": "1.5-Stage Tiny Hourglass",
        "head": "Direct Keypoint Regression",
        "paper": "https://arxiv.org/abs/2006.10204",
        "authors": "Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, Matthias Grundmann",
        "year": "2020",
        "type": "wholebody",
        "variants": {
            "lite": {
                "name": "BlazePose-Lite",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "blazepose",
                "format": "mediapipe",
                "url": "https://drive.google.com/file/d/1lTYBuK2K70Od7c_UjJq33Ap3Wvt_5w9I/view?usp=sharing",
                "source": "MediaPipe",
                "source_url": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/latest/pose_landmarker_lite.task",
                "code": "https://github.com/google/mediapipe",
                "license": "Apache-2.0"
            },
            "full": {
                "name": "BlazePose-Full",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "blazepose",
                "format": "mediapipe",
                "url": "https://drive.google.com/file/d/1ckRy3I5t_vRnPzTrMyl1Ou4UcIH64xiS/view?usp=sharing",
                "source": "MediaPipe",
                "source_url": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/latest/pose_landmarker_full.task",
                "code": "https://github.com/google/mediapipe",
                "license": "Apache-2.0"
            },
            "heavy": {
                "name": "BlazePose-Heavy",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "blazepose",
                "format": "mediapipe",
                "url": "https://drive.google.com/file/d/1LnzU3vq9ezhf3KitNLkV8zkwtXxN8vf3/view?usp=sharing",
                "source": "MediaPipe",
                "source_url": "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/latest/pose_landmarker_heavy.task",
                "code": "https://github.com/google/mediapipe",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "RTMPose",
        "description": "Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. In order to bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multi-person pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-l achieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluate RTMPose's capability in critical real-time applications, we also report the performance after deploying on the mobile device. Our RTMPose-s achieves 72.2% AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing open-source libraries.",
        "pipeline": "Top-down",
        "dimension": "2d",
        "backbone": "CSPNeXt",
        "head": "Coordinate Classification",
        "paper": "https://arxiv.org/abs/2303.07399",
        "authors": "Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, Kai Chen",
        "year": "2023",
        "type": "body2d",
        "variants": {
            "t": {
                "name": "RTMPose-t",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1txyOmtcAFLYa9qSzL-mheJlc5G6iQDjC",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmpose-t-3f713b.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py",
                "license": "Apache-2.0"
            },
            "s": {
                "name": "RTMPose-s",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1dc-yrbQ3oJldfBfCj54oyglO6njT5c78",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmpose-s-903875.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py",
                "license": "Apache-2.0"
            },
            "m": {
                "name": "RTMPose-m",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1na70j7QYud8N4-Pc-3xXS6WUe0NCSEPF",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmpose-m-4d5988.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py",
                "license": "Apache-2.0"
            },
            "l": {
                "name": "RTMPose-l",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1BKGaT4NS3q8KkkoeJX0SfpFEnRbdUKxC",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmpose-l-059cd7.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "RTMW",
        "description": "Whole-body pose estimation is a challenging task that requires simultaneous prediction of keypoints for the body, hands, face, and feet. Whole-body pose estimation aims to predict fine-grained pose information for the human body, including the face, torso, hands, and feet, which plays an important role in the study of human-centric perception and generation and in various applications. In this work, we present RTMW (Real-Time Multi-person Whole-body pose estimation models), a series of high-performance models for 2D/3D whole-body pose estimation. We incorporate RTMPose model architecture with FPN and HEM (Hierarchical Encoding Module) to better capture pose information from different body parts with various scales. The model is trained with a rich collection of open-source human keypoint datasets with manually aligned annotations and further enhanced via a two-stage distillation strategy. RTMW demonstrates strong performance on multiple whole-body pose estimation benchmarks while maintaining high inference efficiency and deployment friendliness. We release three sizes: m/l/x, with RTMW-l achieving a 70.2 mAP on the COCO-Wholebody benchmark, making it the first open-source model to exceed 70 mAP on this benchmark. Meanwhile, we explored the performance of RTMW in the task of 3D whole-body pose estimation, conducting image-based monocular 3D whole-body pose estimation in a coordinate classification manner. We hope this work can benefit both academic research and industrial applications.",
        "pipeline": "Top-down",
        "dimension": "2d",
        "backbone": "CSPNeXt",
        "head": "Coordinate Classification",
        "paper": "https://arxiv.org/abs/2407.08634",
        "authors": "Tao Jiang, Xinchen Xie, Yining Li",
        "year": "2024",
        "type": "wholebody",
        "variants": {
            "wm": {
                "name": "RTMW-m",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco_wholebody",
                "format": "onnx",
                "url": "https://drive.google.com/file/d/1XolCPAIzPQDw55ANzKQ-nQVwfqZRoJdx/view?usp=drive_link",
                "source": "MMPose",
                "source_url": "https://download.openmmlab.com/mmpose/v1/projects/rtmw/onnx_sdk/rtmw-dw-m-s_simcc-cocktail14_270e-256x192_20231122.zip",
                "code": "https://github.com/open-mmlab/mmpose/blob/5a3be9451bdfdad2053a90dc1199e3ff1ea1a409/configs/wholebody_2d_keypoint/rtmpose/cocktail14/rtmw-m_8xb1024-270e_cocktail14-256x192.py",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "RTMDet",
        "description": "detectors_rtmdet-nano_coco-person_320x320",
        "paper": "https://arxiv.org/abs/2212.07784",
        "year": "2022",
        "type": "detectors",
        "variants": {
            "nano": {
                "name": "RTMDet-nano",
                "input_size": [
                    320,
                    320,
                    3
                ],
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1OeK7rlgPpbkyY5NEcqfaa2qT3ILubue-",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmdet-37adb8.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/projects/rtmpose/rtmdet/person/rtmdet_nano_320-8xb32_coco-person.py",
                "license": "Apache-2.0"
            },
            "m": {
                "name": "RTMDet-m",
                "input_size": [
                    640,
                    640,
                    3
                ],
                "format": "onnx",
                "url": "https://drive.google.com/uc?id=1Wqs-vxaEUvUB8TweaoMVKctAlXeK9GSZ",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/rtmdet-97037b.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/projects/rtmpose/rtmdet/person/rtmdet_m_640-8xb32_coco-person.py",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "MoveNet",
        "description": "MoveNet is an ultra fast and accurate model that detects 17 keypoints of a body. The model is offered on TF Hub with two variants, known as Lightning and Thunder. Lightning is intended for latency-critical applications, while Thunder is intended for applications that require high accuracy. Both models run faster than real time (30+ FPS) on most modern desktops, laptops, and phones, which proves crucial for live fitness, sports, and health applications.",
        "pipeline": "Bottom-up",
        "dimension": "2d",
        "backbone": "MobileNetV2 + FPN",
        "head": "Person Center, Keypoint Heatmap, Keypoint Regression, and Offset Heads",
        "paper": "https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html",
        "year": "2021",
        "type": "body2d",
        "variants": {
            "lightning": {
                "name": "MoveNet Lightning",
                "input_size": [
                    192,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/17E_QH9_8bt_DCi2ey08CPDT3-AJGM81P/view?usp=sharing",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            },
            "lightning16": {
                "name": "MoveNet Lightning FP16",
                "input_size": [
                    192,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/1Th9y2AOdQVKhM9OJ65EIsgAHerDtxSpj/view?usp=sharing",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            },
            "lightning8": {
                "name": "MoveNet Lightning INT8",
                "input_size": [
                    192,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/1JbVRFrCvrmC8tE7VX7_v1AKqI12gEUVE/view?usp=sharing",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            },
            "thunder": {
                "name": "MoveNet Thunder",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/1NHIO9hU-8YLreT9TFyMqgjXJWWPMWNwi/view?usp=sharing",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            },
            "thunder16": {
                "name": "MoveNet Thunder FP16",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/1qzbwGjsZ-HHUmr3xFGHEUhsjrU2ewsZ8/view?usp=sharing",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            },
            "thunder8": {
                "name": "MoveNet Thunder Int8",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/file/d/1ZAhBmrm6zu7R1k1AknXT1dsXryu6UMjd/view?usp=drive_link",
                "source": "TensorFlow Hub",
                "source_url": "https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite",
                "code": "",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "EfficientPose with NAS",
        "description": "Human pose estimation from image and video is a vital task in many multimedia applications. Previous methods achieve great performance but rarely take efficiency into consideration, which makes it difficult to implement the networks on resource-constrained devices. Nowadays real-time multimedia applications call for more efficient models for better interactions. Moreover, most deep neural networks for pose estimation directly reuse the networks designed for image classification as the backbone, which are not yet optimized for the pose estimation task. In this paper, we propose an efficient framework targeted at human pose estimation including two parts, the efficient backbone and the efficient head. By implementing the differentiable neural architecture search method, we customize the backbone network design for pose estimation and reduce the computation cost with negligible accuracy degradation. For the efficient head, we slim the transposed convolutions and propose a spatial information correction module to promote the performance of the final prediction. In experiments, we evaluate our networks on the MPII and COCO datasets. Our smallest model has only 0.65 GFLOPs with 88.1% PCKh@0.5 on MPII and our large model has only 2 GFLOPs while its accuracy is competitive with the state-of-the-art large model, i.e., HRNet with 9.5 GFLOPs.",
        "pipeline": "Top-down",
        "dimension": "2d",
        "backbone": "NAS-Optimized Backbone",
        "head": "Heatmap Regression",
        "paper": "https://arxiv.org/abs/2012.07086",
        "year": "2020",
        "type": "body2d",
        "variants": {
            "a17": {
                "name": "NASNet-A (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1I72WYRxa2L_EtakSrQhrSBDdjfKZu6Dw",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1oNr8Ts5EdYwrvV0-COrkYxhmvbjgNwim/view?usp=drive_link",
                "code": "https://github.com/hustvl/EfficientPose/blob/master/experiments/coco/efficientpose/nasnet_192x256_adam_lr1e-3_efficientpose-a.yaml",
                "license": "Unspecified"
            },
            "b17": {
                "name": "NASNet-B (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1wmuudUy4-CDp8RiuBRnNbr0UTcGsqpuo",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1glmyuTcYNxmpivQM5ifF-1lSHUoxmUyP/view?usp=drive_link",
                "code": "https://github.com/hustvl/EfficientPose/blob/master/experiments/coco/efficientpose/nasnet_192x256_adam_lr1e-3_efficientpose-b.yaml",
                "license": "Unspecified"
            },
            "b16": {
                "name": "NASNet-B (MPII)",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1sto7eqJoeFDwcu93g16c3KAd8Ggfcg6X",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1PER2gTZwBQ-KtuFyKVl7277sLuifKDK3/view?usp=drive_link",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py",
                "license": "Unspecified"
            },
            "c17": {
                "name": "NASNet-C (COCO)",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1qAr2fsarwe8uwHFX2tmRRSiKTYNkghfq",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1xUw1CEdiaDQ88vpB1V6rp4juUUoY73vt/view?usp=drive_link",
                "code": "https://github.com/hustvl/EfficientPose/blob/master/experiments/coco/efficientpose/nasnet_192x256_adam_lr1e-3_efficientpose-c.yaml",
                "license": "Unspecified"
            },
            "c16": {
                "name": "NASNet-C (MPII)",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1X-NTpXx0eCV6I6agd1A35bYfpBEqxTLG",
                "source": "Official Repo",
                "source_url": "https://drive.google.com/file/d/1_UuDHAdzTctKjZtGCzRdwPocuF9yn1-1/view?usp=drive_link",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc1/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py",
                "license": "Unspecified"
            }
        }
    },
    {
        "name": "SimCC",
        "description": "The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE) for years due to high performance. However, the long-standing quantization error problem in the 2D heatmap-based methods leads to several well-known drawbacks: 1) The performance for the low-resolution inputs is limited; 2) To improve the feature map resolution for higher localization precision, multiple costly upsampling layers are required; 3) Extra post-processing is adopted to reduce the quantization error. To address these issues, we aim to explore a brand new scheme, called \textit{SimCC}, which reformulates HPE as two classification tasks for horizontal and vertical coordinates. The proposed SimCC uniformly divides each pixel into several bins, thus achieving sub-pixel localization precision and low quantization error. Benefiting from that, SimCC can omit additional refinement post-processing and exclude upsampling layers under certain settings, resulting in a more simple and effective pipeline for HPE. Extensive experiments conducted over COCO, CrowdPose, and MPII datasets show that SimCC outperforms heatmap-based counterparts, especially in low-resolution settings by a large margin.",
        "pipeline": "Top-down",
        "dimension": "2d",
        "backbone": "ResNet, MobileNetV2, ViPNAS-MBV3",
        "head": "Coordinate Classification",
        "paper": "https://arxiv.org/abs/2107.03332",
        "year": "2021",
        "type": "body2d",
        "variants": {
            "mbv2": {
                "name": "SimCC-mbv2",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/file/d/15zJIXQWH5Ui5h0PMcnCCZ2S4JNjCXZtc/view?usp=sharing",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/simcc-0d1bf8.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc0/configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py",
                "license": "Apache-2.0"
            },
            "vipnas": {
                "name": "SimCC-ViPNAS-MBV3",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/file/d/1ia4mkM5ATc2uran5WZWoSeP8N4Q4EWVA/view?usp=sharing",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/simcc-722a37.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc0/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py",
                "license": "Apache-2.0"
            },
            "res50": {
                "name": "SimCC-ResNet50",
                "input_size": [
                    256,
                    192,
                    3
                ],
                "keypoints": "coco",
                "format": "onnx",
                "url": "https://drive.google.com/file/d/1tiLlu6T9JQNYVCev-YDterG8UUfrnIiL/view?usp=sharing",
                "source": "MMDeploy",
                "source_url": "https://mmdeploy-oss.openmmlab.com/model/mmpose/simcc-fefdab.onnx",
                "code": "https://github.com/open-mmlab/mmpose/blob/v1.0.0rc0/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py",
                "license": "Apache-2.0"
            }
        }
    },
    {
        "name": "EfficientPose",
        "description": "Single-person human pose estimation facilitates markerless movement analysis in sports, as well as in clinical applications. Still, state-of-the-art models for human pose estimation generally do not meet the requirements of real-life applications. The proliferation of deep learning techniques has resulted in the development of many advanced approaches. However, with the progresses in the field, more complex and inefficient models have also been introduced, which have caused tremendous increases in computational demands. To cope with these complexity and inefficiency challenges, we propose a novel convolutional neural network architecture, called EfficientPose, which exploits recently proposed EfficientNets in order to deliver efficient and scalable single-person pose estimation. EfficientPose is a family of models harnessing an effective multi-scale feature extractor and computationally efficient detection blocks using mobile inverted bottleneck convolutions, while at the same time ensuring that the precision of the pose configurations is still improved. Due to its low complexity and efficiency, EfficientPose enables real-world applications on edge devices by limiting the memory footprint and computational cost. The results from our experiments, using the challenging MPII single-person benchmark, show that the proposed EfficientPose models substantially outperform the widely-used OpenPose model both in terms of accuracy and computational efficiency. In particular, our top-performing model achieves state-of-the-art accuracy on single-person MPII, with low-complexity ConvNets.",
        "pipeline": "Bottom-up",
        "dimension": "2d",
        "backbone": "EfficientNet",
        "head": "Heatmap and Part Affinity Fields Regression",
        "paper": "https://arxiv.org/abs/2004.12186",
        "authors": "Daniel Groos, Heri Ramampiaro, Espen A. F. Ihlen",
        "year": "2020",
        "type": "body2d",
        "variants": {
            "rt": {
                "name": "EfficientPose-RT",
                "input_size": [
                    224,
                    224,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1H_tWXQUWPEELbU2rDaG6e4Oa3aX2E_-t",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseRT.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "rt_lite": {
                "name": "EfficientPose-RT-Lite",
                "input_size": [
                    224,
                    224,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=13SQSQUf6B-s_2EcDjTDRNy1Hq7tIMxFy",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseRT_LITE.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "i": {
                "name": "EfficientPose-I",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=17hqzQUNeBCV7qHjzirBteygwBKQNVcmA",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseI.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "i_lite": {
                "name": "EfficientPose-I-Lite",
                "input_size": [
                    256,
                    256,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1N33Js3JwaApd1SvZLZIYdkBH43WzA5tc",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseI_LITE.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "ii": {
                "name": "EfficientPose-II",
                "input_size": [
                    368,
                    368,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1oAyJuuyKPellwDL9evualaG3FXaaAthl",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseII.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "ii_lite": {
                "name": "EfficientPose-II-Lite",
                "input_size": [
                    368,
                    368,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1o-AYZMejDddJ8ELjtN_Xb6r7tcp9O0Kq",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseII_LITE.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "iii": {
                "name": "EfficientPose-III",
                "input_size": [
                    480,
                    480,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1b7BdOyhdOmCARq3xqacQTy6hs-eUx85v",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseIII.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            },
            "iv": {
                "name": "EfficientPose-IV",
                "input_size": [
                    600,
                    600,
                    3
                ],
                "keypoints": "mpii",
                "format": "tflite",
                "url": "https://drive.google.com/uc?id=1vMELKDkF-pogSgOi1XrgMm7Vjn0iGXG3",
                "source": "Official Repo",
                "source_url": "https://github.com/daniegr/EfficientPose/blob/master/models/tflite/EfficientPoseIV.tflite",
                "code": "https://github.com/daniegr/EfficientPose",
                "license": "Apache-2.0"
            }
        }
    }
]